{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "  plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    version=1.0.0,\n",
      "    description='Large Movie Review Dataset.\n",
      "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "        'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
      "    }),\n",
      "    total_num_examples=100000,\n",
      "    splits={\n",
      "        'test': 25000,\n",
      "        'train': 25000,\n",
      "        'unsupervised': 50000,\n",
      "    },\n",
      "    supervised_keys=('text', 'label'),\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n",
      "encoder <SubwordTextEncoder vocab_size=8185>\n",
      "info.features FeaturesDict({\n",
      "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
      "})\n",
      "Vocabulary size: 8185\n",
      "Encoded string is [4025, 222, 6307, 2327, 4043, 2120, 7975]\n",
      "The original string: \"Hello TensorFlow.\"\n",
      "4025 ----> Hell\n",
      "222 ----> o \n",
      "6307 ----> Ten\n",
      "2327 ----> sor\n",
      "4043 ----> Fl\n",
      "2120 ----> ow\n",
      "7975 ----> .\n"
     ]
    }
   ],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "print(\"info\",info)\n",
    "print(\"encoder\",encoder)\n",
    "print(\"info.features\",info.features)\n",
    "print('Vocabulary size: {}'.format(encoder.vocab_size))\n",
    "sample_string = 'Hello TensorFlow.'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print('Encoded string is {}'.format(encoded_string))\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print('The original string: \"{}\"'.format(original_string))\n",
    "\n",
    "for index in encoded_string:\n",
    "  print('{} ----> {}'.format(index, encoder.decode([index])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  62   18   41  604  927   65    3  644 7968   21   35 5096   36   11\n",
      "   43 2948 5240  102   50  681 7862 1244    3 3266   29  122  640    2\n",
      "   26   14  279  438   35   79  349  384   11 1991    3  492   79  122\n",
      "  188  117   33 4047 4531   14   65 7968    8 1819 3947    3   62   27\n",
      "    9   41  577 5044 2629 2552 7193 7961 3642    3   19  107 3903  225\n",
      "   85  198   72    1 1512  738 2347  102 6245    8   85  308   79 6936\n",
      " 7961   23 4981 8044    3 6429 7961 1141 1335 1848 4848   55 3601 4217\n",
      " 8050    2    5   59 3831 1484 8040 7974  174 5773   22 5240  102   18\n",
      "  247   26    4 3903 1612 3902  291   11    4   27   13   18 4092 4008\n",
      " 7961    6  119  213 2774    3   12  258 2306   13   91   29  171   52\n",
      "  229    2 1245 5790  995 7968    8   52 2948 5240 8039 7968    8   74\n",
      " 1249    3   12  117 2438 1369  192   39 7975], shape=(163,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[  12   31   93  867    7 1256 6585 7961  421  365    2   26   14    9\n",
      "  988 1089    7    4 6728    6  276 5760 2587    2   81 6118 8029    2\n",
      "  139 1892 7961    5 5402  246   25    1 1771  350    5  369   56 5397\n",
      "  102    4 2547    3 4001   25   14 7822  209   12 3531 6585 7961   99\n",
      "    1   32   18 4762    3   19  184 3223   18 5855 1045    3 4232 3337\n",
      "   64 1347    5 1190    3 4459    8  614    7 3129    2   26   22   84\n",
      " 7020    6   71   18 4924 1160  161   50 2265    3   12 3983    2   12\n",
      "  264   31 2545  261    6    1   66    2   26  131  393    1 5846    6\n",
      "   15    5  473   56  614    7 1470    6  116  285 4755 2088 7961  273\n",
      "  119  213 3414 7961   23  332 1019    3   12 7667  505   14   32   44\n",
      "  208 7975], shape=(142,), dtype=int64)\n",
      "<PrefetchDataset shapes: ((None,), ()), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "iterator=iter(train_dataset)\n",
    "for _ in range(2):\n",
    "  text,_=next(iterator)\n",
    "  print(text)\n",
    "print(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "# history = model.fit(train_dataset, epochs=10,\n",
    "#                     validation_data=test_dataset,\n",
    "#                     validation_steps=30)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00208139]]\n"
     ]
    }
   ],
   "source": [
    "def pad_to_size(vec, size):\n",
    "  zeros = [0] * (size - len(vec))\n",
    "  vec.extend(zeros)\n",
    "  return vec\n",
    "def sample_predict(sample_pred_text, pad):\n",
    "  encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
    "\n",
    "  if pad:# 短于64的话补充0，长过64的话不管\n",
    "    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
    "  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
    "  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
    "\n",
    "  return (predictions)\n",
    "\n",
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.'\n",
    "                    'were out of this world. I would recommend this movie.'\n",
    "                    'were out of this world. I would recommend this movie.'\n",
    "                    'were out of this world. I would recommend this movie.'\n",
    "                    'were out of this world. I would recommend this movie.'\n",
    "                    'were out of this world. I would recommend this movie.'\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "encoded_sample_pred_text = encoder.encode(sample_pred_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "95"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_sample_pred_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}