{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q tfds-nightly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "outputs": [],
   "source": [
    "\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "outputs": [],
   "source": [
    "one_pt,one_en=next(iter(train_examples))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_datasets' has no attribute 'deprecated'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0mTraceback (most recent call last)",
      "\u001B[0;32m<ipython-input-390-bcab781f0508>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# 这个的功能除了分词，当遇到oov的时候相当于把词做切分了，比如Transformer就可能变成Trans和fromer两个词\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n\u001B[0m\u001B[1;32m      5\u001B[0m     (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow_datasets' has no attribute 'deprecated'"
     ]
    }
   ],
   "source": [
    "# 这个类库不知道为啥我用不了，我换成tokenizer了\n",
    "# 这个的功能除了分词，当遇到oov的时候相当于把词做切分了，比如Transformer就可能变成Trans和fromer两个词\n",
    "# The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary.\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "outputs": [],
   "source": [
    "# 只保留2**13个词 八千多\n",
    "tokenizer_en=tf.keras.preprocessing.text.Tokenizer(num_words=2**13,oov_token=\"<UNK>\")\n",
    "tokenizer_en.fit_on_texts([en.numpy().decode('utf-8') for pt, en in train_examples])\n",
    "tokenizer_pt=tf.keras.preprocessing.text.Tokenizer(num_words=2**13,oov_token=\"<UNK>\")\n",
    "tokenizer_pt.fit_on_texts([pt.numpy().decode('utf-8') for pt, en in train_examples])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [[1, 13, 1694]]\n",
      "The original string: ['<UNK> is awesome']\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.texts_to_sequences([sample_string])\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.sequences_to_texts(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [
    {
     "data": {
      "text/plain": "([8192, 5, 35, 1, 2, 761, 3901, 2, 358, 2257, 18, 1702, 4, 8, 2, 1, 8193],\n [8192,\n  3,\n  53,\n  12,\n  1085,\n  1,\n  12,\n  86,\n  117,\n  237,\n  2,\n  39,\n  2009,\n  5,\n  2286,\n  84,\n  13,\n  1,\n  8193])"
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def encode(lang1, lang2):\n",
    "  # 与官网不同，我做了修改，使用的是tokenizer\n",
    "  lang1 = [2**13] + tokenizer_pt.texts_to_sequences(\n",
    "      [lang1.numpy().decode(\"utf8\")])[0] + [2**13+1]\n",
    "\n",
    "  lang2 = [2**13] + tokenizer_en.texts_to_sequences(\n",
    "      [lang2.numpy().decode(\"utf8\")])[0] + [2**13+1]\n",
    "\n",
    "  return lang1, lang2\n",
    "\n",
    "def tf_encode(pt, en):\n",
    "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "  result_pt.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_pt, result_en\n",
    "\n",
    "MAX_LENGTH = 40\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "  return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)\n",
    "\n",
    "encode(one_pt,one_en)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(64, 39), dtype=int64, numpy=\n array([[8192,  282, 7964, ...,    0,    0,    0],\n        [8192,   87,  136, ...,    0,    0,    0],\n        [8192, 4239,    9, ...,    0,    0,    0],\n        ...,\n        [8192,   87,    2, ...,    0,    0,    0],\n        [8192,   26, 7434, ...,    0,    0,    0],\n        [8192,   21, 1247, ...,    0,    0,    0]])>,\n <tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n array([[8192,   85,   18, ...,    0,    0,    0],\n        [8192,    8,   19, ...,    0,    0,    0],\n        [8192,    8, 2251, ...,    0,    0,    0],\n        ...,\n        [8192,    9,   19, ...,    0,    0,    0],\n        [8192,    8, 1562, ...,    0,    0,    0],\n        [8192,  941,   10, ...,    0,    0,    0]])>)"
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "outputs": [],
   "source": [
    "# tokenizer_pt.word_docs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(50, 512)\n",
      "tf.Tensor(\n",
      "[[[ 0.          1.          0.         ...  1.          0.\n",
      "    1.        ]\n",
      "  [ 0.84147096  0.5403023   0.8218562  ...  1.          0.00010366\n",
      "    1.        ]\n",
      "  [ 0.9092974  -0.41614684  0.9364147  ...  1.          0.00020733\n",
      "    1.        ]\n",
      "  ...\n",
      "  [ 0.12357312 -0.9923355   0.97718984 ...  0.99998724  0.00487216\n",
      "    0.99998814]\n",
      "  [-0.76825464 -0.64014435  0.7312359  ...  0.9999867   0.00497582\n",
      "    0.9999876 ]\n",
      "  [-0.95375264  0.30059254 -0.14402692 ...  0.9999861   0.00507948\n",
      "    0.9999871 ]]], shape=(1, 50, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "终于通过代码看懂了Positional encoding是个啥\n",
    "相当于对位置信息进行embedding，假如说有一个长度为50的序列，那么对这个序列的每个位置进行编码，假如需要的维度为512\n",
    "对于其第pos个位置（例如第0个位置，pos就是0），先计算一个pos/10000^{2*i/512}的向量，i从0一直到511，所以这是一个512维度的向量\n",
    "然后如果这个pos是一个奇数，就对其计算cos，否则计算sin\n",
    "最终的position encoding就是一个50*512的向量\n",
    "\"\"\"\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "  print(pos.shape)\n",
    "  print(i.shape)\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  print(angle_rates.shape)\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  print(angle_rads.shape)\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\narray([[[[0., 0., 1., 1., 0.]]],\n\n\n       [[[0., 0., 0., 1., 1.]]],\n\n\n       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[0., 1., 1.],\n       [0., 0., 1.],\n       [0., 0., 0.]], dtype=float32)>"
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  # 这是个有趣的api，相当于对一个矩阵斜着进行遮罩\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "  这代码写得真好，看明白了，mask的形状不定，只要能与(..., seq_len_q, seq_len_k)相加就好了\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "  # q和k的最后两维度被当做矩阵进行相乘\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    # 当mask为1的地方，这个attention是无限小的，而由于激活函数是relu，所以导致这些地方的输出也是0，从而保证未来的词或者padding词不会对之前的信息产生影响\n",
    "    # 并且反向传播传递不回去，不会对之前的0位置产生梯度\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n",
      "牛逼\n",
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n",
      "牛逼\n",
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n",
      "牛逼\n"
     ]
    }
   ],
   "source": [
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)\n",
    "  \n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)\n",
    "\n",
    "print(\"牛逼\")\n",
    "\n",
    "# This query aligns equally with the first and second key,\n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)\n",
    "print(\"牛逼\")\n",
    "\n",
    "# This query aligns with a repeated key (third and fourth),\n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)\n",
    "print(\"牛逼\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3, 2), dtype=float32, numpy=\narray([[[[0., 1.],\n         [0., 1.],\n         [0., 1.]],\n\n        [[0., 1.],\n         [0., 1.],\n         [0., 1.]]],\n\n\n       [[[1., 0.],\n         [1., 0.],\n         [1., 0.]],\n\n        [[1., 0.],\n         [1., 0.],\n         [1., 0.]]]], dtype=float32)>"
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=tf.constant([[[[0,1]]],[[[1,0]]]],dtype=tf.float32)\n",
    "print(mask.shape)\n",
    "tf.zeros((2,2,3,2))+mask\n",
    "# scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "# temp_k = tf.constant([[10,0,0],\n",
    "#                       [0,10,0],\n",
    "#                       [0,0,10],\n",
    "#                       [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "#\n",
    "# temp_v = tf.constant([[   1,0],\n",
    "#                       [  10,0],\n",
    "#                       [ 100,5],\n",
    "#                       [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "#\n",
    "# # This `query` aligns with the second `key`,\n",
    "# # so the second `value` is returned.\n",
    "# temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "#\n",
    "# temp_out, temp_attn = scaled_dot_product_attention(\n",
    "#       temp_k, temp_v, temp_q, [])\n",
    "# print ('Attention weights are:')\n",
    "# print (temp_attn)\n",
    "# print ('Output is:')\n",
    "# print (temp_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    这个玩意的功能就比如说，输入的是(batch_size,seq_len,d_model)，然后把最后一个depth分割成(batch_size,num_heads,seq_len,depth)\n",
    "    reshape的-1代表自动计算\n",
    "    最后这个transpose就是为了把head挪到1轴去，通过学习transpose可知，这个操作就相当于把d_model切开了\n",
    "    意义在于：结果的里每个head（最后两个维度对应的数据）保存的是全部sequence对应的全部d_model中的一块（就是一个head）\n",
    "    不要去想具体是怎么转置的，维度高了想不明白的，只要知道就是轴变换就好\n",
    "    \"\"\"\n",
    "    # print(x.shape)\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "outputs": [
    {
     "data": {
      "text/plain": "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [3,2], In[1]: [512,512] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0mTraceback (most recent call last)",
      "\u001B[0;32m<ipython-input-405-5dc3aac95758>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muniform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtemp_mha\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    983\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    984\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menable_auto_cast_variables\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compute_dtype_object\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 985\u001B[0;31m           \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    986\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    987\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-403-87aa0afaa667>\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, v, k, q, mask)\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[0mbatch_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m     \u001B[0mq\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# (batch_size, seq_len, d_model)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m     \u001B[0mk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# (batch_size, seq_len, d_model)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m     \u001B[0mv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# (batch_size, seq_len, d_model)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    983\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    984\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menable_auto_cast_variables\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compute_dtype_object\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 985\u001B[0;31m           \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    986\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    987\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1197\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mactivation\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1198\u001B[0;31m         dtype=self._compute_dtype_object)\n\u001B[0m\u001B[1;32m   1199\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1200\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mcompute_output_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/ops/core.py\u001B[0m in \u001B[0;36mdense\u001B[0;34m(inputs, kernel, bias, activation, dtype)\u001B[0m\n\u001B[1;32m     54\u001B[0m   \u001B[0;31m# Broadcast kernel to inputs.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 56\u001B[0;31m     \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstandard_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensordot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkernel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrank\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     57\u001B[0m     \u001B[0;31m# Reshape the output back to the original ndim of the input.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    199\u001B[0m     \u001B[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 201\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    202\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m       \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mtensordot\u001B[0;34m(a, b, axes, name)\u001B[0m\n\u001B[1;32m   4517\u001B[0m     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\n\u001B[1;32m   4518\u001B[0m         b, b_axes, True)\n\u001B[0;32m-> 4519\u001B[0;31m     \u001B[0mab_matmul\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmatmul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma_reshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb_reshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4520\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma_free_dims\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb_free_dims\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4521\u001B[0m       if (ab_matmul.get_shape().is_fully_defined() and\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    199\u001B[0m     \u001B[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 201\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    202\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m       \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mmatmul\u001B[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001B[0m\n\u001B[1;32m   3253\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3254\u001B[0m       return gen_math_ops.mat_mul(\n\u001B[0;32m-> 3255\u001B[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001B[0m\u001B[1;32m   3256\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3257\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36mmat_mul\u001B[0;34m(a, b, transpose_a, transpose_b, name)\u001B[0m\n\u001B[1;32m   5622\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5623\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5624\u001B[0;31m       \u001B[0m_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   5625\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5626\u001B[0m       \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mraise_from_not_ok_status\u001B[0;34m(e, name)\u001B[0m\n\u001B[1;32m   6841\u001B[0m   \u001B[0mmessage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmessage\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\" name: \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6842\u001B[0m   \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6843\u001B[0;31m   \u001B[0msix\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_status_to_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6844\u001B[0m   \u001B[0;31m# pylint: enable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6845\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001B[0m in \u001B[0;36mraise_from\u001B[0;34m(value, from_value)\u001B[0m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: Matrix size-incompatible: In[0]: [3,2], In[1]: [512,512] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "y = tf.random.uniform((1, 3, 2))\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=[])\n",
    "out.shape, attn.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "print(sample_ffn(tf.random.uniform((64, 50, 512))).shape)\n",
    "sample_ffn.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, mask):\n",
    "    \"\"\"\n",
    "\n",
    "    :param x: tensor 形如[batch_size,seq_len,emb_size] 这里要求的emb_size就是d_model\n",
    "    :param training:\n",
    "    :param mask:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([64, 43, 512])"
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "这一段数据是怎么流动的，输入是形如batch_size,seq_len,emb_size\n",
    "这个东西会喂给MultiHeadAttention做self attention，而由于split——head的存在，MultiHeadAttention会把输入进行处理\n",
    "那么每个scale-dot-production的输入就是(batch_size,head_num,seq_len,seq_len)\n",
    "计算出来的QK^T形状是(batch_size,head_num,seq_len,seq_len)\n",
    "然后做mask，如果这个mask之后再说\n",
    "这些个multiHeadAttention输出的结果也是一个(batch_size, seq_len, emb_size)\n",
    "\"\"\"\n",
    "\n",
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "  def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "    # 这俩mask的实际应用就是在scaled_dot_product_attention里，给输出加了一点小扰动，为啥不知道\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([64, 50, 512])"
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    # emb层的维度就是d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                            self.d_model) #输出就是一个maximum_position_encoding*d_model的数组\n",
    "\n",
    "    # 多个嵌入层\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "\n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # 为啥要乘以一个根号model呢？\n",
    "    x += self.pos_encoding[:, :seq_len, :] #取出pos_encoding的需要用到的长度，也就是输入序列的长度\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask) # 多个嵌入层串联\n",
    "\n",
    "    return x  # (batch_size, input_seq_len, d_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(10000, 512)\n",
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "这一段里的数据是怎么流动的\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      # 多个dec_layer也是串联，但是每个dec_layer都用到了enc的输出\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "\n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(5000, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input,\n",
    "                              enc_output=sample_encoder_output,\n",
    "                              training=False,\n",
    "                              look_ahead_mask=None,\n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inp, tar, training, enc_padding_mask,\n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "    return final_output, attention_weights\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(10000, 512)\n",
      "(6000, 1)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(6000, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": "TensorShape([64, 36, 8192])"
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
    "    input_vocab_size=8500, target_vocab_size=2**13, # 这个我改了一下，因为我设置的词表大小是2**13\n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n",
    "                               enc_padding_mask=None,\n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "# input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "input_vocab_size = 2**13 + 2\n",
    "# target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "target_vocab_size = 2**13 + 2\n",
    "dropout_rate = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 0, 'Train Step')"
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8194, 1)\n",
      "(1, 128)\n",
      "(1, 128)\n",
      "(8194, 128)\n",
      "(8194, 1)\n",
      "(1, 128)\n",
      "(1, 128)\n",
      "(8194, 128)\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size,\n",
    "                          pe_input=input_vocab_size,\n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)\n",
    "def create_masks(inp, tar):\n",
    "  # (batch_size, seq_len)\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp) # 这个功能就是找pading，是padding位置的就会是1，形状与输入相同\n",
    "\n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by\n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1]) # 这个是一个(tf.shape(tar)[1],tf.shape(tar)[1])的方形数组\n",
    "  dec_target_padding_mask = create_padding_mask(tar) #这是个(batch_size, 1, 1, seq_len)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask) # 这个的输出是(batch_size, 1, seq_len, seq_len)\n",
    "  # 即会扩充\n",
    "  \"\"\"\n",
    "  这些个musk是怎么使用的呢\n",
    "  先举个小栗子\n",
    "  比如说tar的其中一个序列是[1,2,2,0,0]\n",
    "  那么look_ahead_mask就是一个[[0,1,1,1,1],[0,0,1,1,1],[0,0,0,1,1],[0,0,0,0,1],[0,0,0,0,0]] seq_len,seq_len大小\n",
    "  dec_target_padding_mask就是[0,0,0,1,1]，即确定padding位置\n",
    "  于是combined_mask结果就是[[0,1,1,1,1],[0,0,1,1,1],[0,0,0,1,1],[0,0,0,1,1],[0,0,0,1,1]]\n",
    "  这是为了保证padding的位置不需要进行lookahead masking\n",
    "  这些个mask会被直接送进各个scale dot production里\n",
    "  \n",
    "  这些东西怎么用呢\n",
    "  以enc_padding_mask为例，在实际应用中，其形状为[batch_size,1,1,seq_len] 记为A。\n",
    "  以encoder部分的multiheadattention为例，multiheadattention里scale dot product接受的是[batch_size,num_head,seq_len,depth]\n",
    "  乘积项QK^T刚好也是一个形状[batch_size,num_head,seq_len,seq_len] 记为B\n",
    "  B+A*-10^9的时候，会对A做broadcast，也就是说，假如A[0,0,0,10]=1，那么结果的[0,:,:,10]都等于一个很小的负数\n",
    "  这样做的意义也出现了，就是不要去计算pad与其他词之间的self attention\n",
    "\n",
    "\n",
    "  注意，combined_mask只在decode使用\n",
    "  encoder只使用了padding_mask。因为encoding阶段不需要考虑look ahead啊\n",
    "  combined_mask的形状是(batch_size,1,seq_len,seq_len)，而在scale dot里，QK^T是（batch_size,num_head,seq_len,seq_len)大小的\n",
    "  两者刚好可以相加，与上面道理类似\n",
    "  \"\"\"\n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n",
      "tf.Tensor([[[[0. 0. 0. 1. 1.]]]], shape=(1, 1, 1, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 1. 1.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "look_ahead_mask = create_look_ahead_mask(5)\n",
    "dec_target_padding_mask = create_padding_mask([[1,2,2,0,0]]) #这是个(batch_size, 1, 1, seq_len)\n",
    "combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "print(look_ahead_mask)\n",
    "print(dec_target_padding_mask)\n",
    "print(combined_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 1, 5, 5), dtype=float32, numpy=\narray([[[[0., 1., 1., 1., 1.],\n         [0., 0., 1., 1., 1.],\n         [0., 0., 0., 1., 1.],\n         [0., 0., 0., 0., 1.],\n         [0., 0., 0., 0., 0.]]],\n\n\n       [[[0., 1., 2., 3., 4.],\n         [0., 1., 2., 3., 4.],\n         [0., 1., 2., 3., 4.],\n         [0., 1., 2., 3., 4.],\n         [0., 1., 2., 3., 4.]]]], dtype=float32)>"
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.maximum(tf.reshape(tf.range(-5,5,dtype=tf.float32),(2,1,1,5)), create_look_ahead_mask(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\narray([[0., 1., 1., 1., 1.],\n       [0., 0., 1., 1., 1.],\n       [0., 0., 0., 1., 1.],\n       [0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 0.]], dtype=float32)>"
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_look_ahead_mask(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "\n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  # print(enc_padding_mask)\n",
    "  # print(combined_mask)\n",
    "  # print(dec_padding_mask)\n",
    "  # return\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp,\n",
    "                                 True,\n",
    "                                 enc_padding_mask,\n",
    "                                 combined_mask,\n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "\n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "    break\n",
    "  break\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "\n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                train_loss.result(),\n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [],
   "source": [
    "inp_batch, tar_batch=next(iter(train_dataset))\n",
    "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp_batch, tar_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(64, 1, 1, 33), dtype=float32, numpy=\narray([[[[0., 0., 0., ..., 0., 0., 1.]]],\n\n\n       [[[0., 0., 0., ..., 1., 1., 1.]]],\n\n\n       [[[0., 0., 0., ..., 1., 1., 1.]]],\n\n\n       ...,\n\n\n       [[[0., 0., 0., ..., 1., 1., 1.]]],\n\n\n       [[[0., 0., 0., ..., 1., 1., 1.]]],\n\n\n       [[[0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>"
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "所以数据到底是怎么变的呢\n",
    "输入数据是(batch_size,seq_len,)\n",
    "\"\"\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}