{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'word2vec/variables/embeddings:0' shape=(5, 2) dtype=float32>, <tf.Variable 'word2vec/variables/nce_weights:0' shape=(5, 2) dtype=float32_ref>, <tf.Variable 'word2vec/variables/nce_biases:0' shape=(5,) dtype=float32_ref>]\n",
      "[[-0.5325353   0.10459913]\n",
      " [ 0.34093955  0.35427415]\n",
      " [ 0.11339732 -0.5679613 ]\n",
      " [ 0.46859023  0.3219796 ]\n",
      " [ 0.1926802  -0.19246615]]\n",
      "[0. 0. 0. 0. 0.]\n",
      "[[(IndexedSlicesValue(values=array([[0.10771506, 0.11192796],\n",
      "       [0.13088524, 0.18375723]], dtype=float32), indices=array([1, 3], dtype=int32), dense_shape=array([5, 2], dtype=int32)), array([[-0.04970081,  0.03095512],\n",
      "       [ 0.02433792, -0.01457626],\n",
      "       [ 0.0433158 , -0.00957564],\n",
      "       [-0.0334233 ,  0.00683755],\n",
      "       [ 0.03496585,  0.01258318]], dtype=float32)), (IndexedSlicesValue(values=array([[-0.00223987,  0.00134148],\n",
      "       [ 0.00232732, -0.00047611],\n",
      "       [-0.00367604, -0.00316339]], dtype=float32), indices=array([1, 2, 1]), dense_shape=array([5, 2], dtype=int32)), array([[-0.5325353 ,  0.10459913],\n",
      "       [ 0.34685546,  0.35609606],\n",
      "       [ 0.11107001, -0.56748515],\n",
      "       [ 0.46859023,  0.3219796 ],\n",
      "       [ 0.1926802 , -0.19246615]], dtype=float32)), (IndexedSlicesValue(values=array([-0.09203201, -0.06963155,  0.81502336], dtype=float32), indices=array([1, 2, 1]), dense_shape=array([5], dtype=int32)), array([ 0.        , -0.72299135,  0.06963155,  0.        ,  0.        ],\n",
      "      dtype=float32))], None]\n",
      "[[-0.5325353   0.10459913]\n",
      " [ 0.34685546  0.35609606]\n",
      " [ 0.11107001 -0.56748515]\n",
      " [ 0.46859023  0.3219796 ]\n",
      " [ 0.1926802  -0.19246615]]\n",
      "[ 0.         -0.72299135  0.06963155  0.          0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\n这是我写word2vec的一个例子，我把minimize拆开成为了计算梯度与应用梯度\\n按理来讲，gradient是一个list，每个元素都是一个tuple，tuple[0]是梯度，tuple[1]是被计算梯度的变量，注意这个变量还没有被梯度下降，也就是梯度下降之前的值\\n但是当我运行了\\n'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#各个optimizer都差不太多，因此归并到同一个ipynb了\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, vocabulary_size, emb_dim, base_scope_name=\"word2vec\"):\n",
    "        self.trainable_variables = list()\n",
    "        self.output_variables = list()\n",
    "        self.losses = list()\n",
    "        self.scopes = list()\n",
    "        self.base_scope_name = base_scope_name\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        scope = \"{0}/variables\".format(self.base_scope_name)\n",
    "        self.scopes.append(scope)\n",
    "        with tf.variable_scope(name_or_scope=scope):\n",
    "            self.scope = tf.get_variable_scope()\n",
    "            self.emb_layer = tf.keras.layers.Embedding(input_dim=vocabulary_size, output_dim=emb_dim,\n",
    "                                                       name=\"emb_layer\")\n",
    "            # keras的layer均为懒加载，仅定义阶段不会构造权重，使用build来手动触发构造，该版本（v1.12.3）下tf.keras.layers.Embedding的build输入不起作用，随便给一个None\n",
    "            self.emb_layer.build(input_shape=None)\n",
    "            self.trainable_variables.extend(self.emb_layer.trainable_variables)\n",
    "\n",
    "    def __call__(self, input_batch, *args, **kwargs):\n",
    "        scope = \"{0}/outputs\".format(self.base_scope_name)\n",
    "        self.scopes.append(scope)\n",
    "        with tf.name_scope(scope):\n",
    "            emb_output = self.emb_layer(input_batch)\n",
    "            self.output_variables.append(emb_output)\n",
    "        return emb_output\n",
    "\n",
    "    def get_nce_loss(self, input_batch, label_batch, num_sampled, label_true_num=1):\n",
    "        emb_output = self(input_batch)\n",
    "        scope = \"{0}/variables\".format(self.base_scope_name)\n",
    "        self.scopes.append(scope)\n",
    "        with tf.variable_scope(name_or_scope=scope):\n",
    "            nce_weights = tf.get_variable(name=\"nce_weights\", shape=[self.vocabulary_size, self.emb_dim],\n",
    "                                          initializer=tf.truncated_normal_initializer(\n",
    "                                              stddev=1.0 / math.sqrt(self.vocabulary_size)))\n",
    "            nce_biases = tf.get_variable(name=\"nce_biases\", shape=[self.vocabulary_size, ],\n",
    "                                         initializer=tf.zeros_initializer(), dtype=tf.float32)\n",
    "            self.trainable_variables.append(nce_weights)\n",
    "            self.trainable_variables.append(nce_biases)\n",
    "\n",
    "        scope = \"{}/losses\".format(self.base_scope_name)\n",
    "        self.scopes.append(scope)\n",
    "        with tf.name_scope(scope):\n",
    "            nce_loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(\n",
    "                    weights=nce_weights, biases=nce_biases, labels=label_batch, inputs=emb_output,\n",
    "                    num_sampled=num_sampled, num_classes=self.vocabulary_size, num_true=label_true_num))\n",
    "            self.losses.append(nce_loss)\n",
    "        return nce_loss\n",
    "\n",
    "word2vec=Word2Vec(5,2)\n",
    "input_ds_sample=tf.data.Dataset.from_tensor_slices([[1.,3.],[3.,4.]])\n",
    "# label_ds_sample=tf.data.Dataset.from_tensor_slices([[[1,2],[2,3]]])\n",
    "label_ds_sample=tf.data.Dataset.from_tensor_slices([[[1],[2]]])\n",
    "\n",
    "\n",
    "input_batch_sample=input_ds_sample.make_one_shot_iterator().get_next()\n",
    "label_batch_sample=label_ds_sample.make_one_shot_iterator().get_next()\n",
    "output_batch_sample=word2vec(input_batch_sample)\n",
    "loss=word2vec.get_nce_loss(input_batch_sample,label_batch_sample,1,1)\n",
    "\n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(1)\n",
    "\n",
    "gradient=optimizer.compute_gradients(loss)\n",
    "apply=optimizer.apply_gradients(gradient)\n",
    "print(tf.trainable_variables())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(tf.trainable_variables()[1]))\n",
    "    print(sess.run(tf.trainable_variables()[-1]))\n",
    "    print(sess.run([gradient,apply]))\n",
    "    # print(sess.run([loss,gradient]))\n",
    "    print(sess.run(tf.trainable_variables()[1]))\n",
    "    print(sess.run(tf.trainable_variables()[-1]))\n",
    "\n",
    "\"\"\"\n",
    "这是我写word2vec的一个例子，我把minimize拆开成为了计算梯度与应用梯度\n",
    "按理来讲，gradient是一个list，每个元素都是一个tuple，tuple[0]是梯度，tuple[1]是被计算梯度的变量，注意这个变量还没有被梯度下降，也就是梯度下降之前的值\n",
    "但是当我运行了print(sess.run([gradient,apply]))的时候，会发现最后一个nce_bias变量变成了梯度下降之后的结果\n",
    "并且，高潮是，结果仍然是正确的，所有的变量都被正确地梯度下降了，nce_bias并没有被梯度下降两次\n",
    "但如果我只运行print(sess.run([loss,gradient]))，打印出来的结果又都是正常的了，即nce_bias就是一堆0\n",
    "\n",
    "我思考了一下为什么会出现这个结果，我给出的猜想是，\n",
    "sess.run会先触发graph上的计算，当graph上的计算都完成了，才把值!同时地!返回回来，也就是说，打印的gradient是apply运行之后的结果了。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor 'IteratorGetNext:0' shape=(2,) dtype=float32>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}